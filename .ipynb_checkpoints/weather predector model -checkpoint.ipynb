{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pickle as pk\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import joblib\n",
    "from sklearn.metrics import (\n",
    "            accuracy_score,\n",
    "            precision_score, \n",
    "            recall_score,\n",
    "            f1_score,\n",
    "            roc_auc_score,\n",
    "            confusion_matrix,\n",
    "            precision_recall_curve,\n",
    "            auc,\n",
    "            mean_absolute_error,\n",
    "            mean_squared_error,\n",
    "            r2_score)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"This Pipeline instance is not fitted yet\")\n",
    "\n",
    "class GLDASFetcher:\n",
    "    def __init__(self, username=None, password=None):\n",
    "        \"\"\"\n",
    "        Initialize GLDAS/POWER fetcher (username/password only needed for GLDAS OPeNDAP, \n",
    "        not for NASA POWER API).\n",
    "        \"\"\"\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "\n",
    "        # GLDAS variable names mapped to NASA POWER parameters\n",
    "        self.variables_map = {\n",
    "            'temp': ['T2M_MAX', 'T2M_MIN'],           # Daily max/min temp\n",
    "            'humidity': ['QV2M'],                     # Specific humidity\n",
    "            'pressure': ['PS'],                       # Surface pressure\n",
    "            'precipitation': ['PRECTOTCORR'],         # Corrected daily precipitation\n",
    "            'solar_rad': ['ALLSKY_SFC_SW_DWN'],       # Solar radiation\n",
    "            'wind_speed': ['WS2M']                    # Wind speed\n",
    "        }\n",
    "\n",
    "    def get_data(self, lat, lon, start_date, end_date, variables=None):\n",
    "        \"\"\"\n",
    "        Fetch daily NASA POWER data for a given location & date range.\n",
    "        \"\"\"\n",
    "        variables = variables or list(self.variables_map.keys())\n",
    "    \n",
    "        base_url = \"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
    "    \n",
    "        # Collect POWER parameter codes\n",
    "        power_params = []\n",
    "        for var in variables:\n",
    "            power_params.extend(self.variables_map.get(var, []))\n",
    "    \n",
    "        params = {\n",
    "            'parameters': ','.join(power_params),\n",
    "            'community': 'RE',\n",
    "            'longitude': lon,\n",
    "            'latitude': lat,\n",
    "            'start': start_date.replace(\"-\", \"\"),\n",
    "            'end': end_date.replace(\"-\", \"\"),\n",
    "            'format': 'JSON'\n",
    "        }\n",
    "    \n",
    "        print(f\"üåç Fetching NASA POWER data for ({lat}, {lon}) from {start_date} to {end_date} ...\")\n",
    "        response = requests.get(base_url, params=params, timeout=60)\n",
    "    \n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå API error {response.status_code}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "        data = response.json()['properties']['parameter']\n",
    "    \n",
    "        # Build dataframe manually\n",
    "        records = {}\n",
    "        for var, timeseries in data.items():\n",
    "            for date_str, value in timeseries.items():\n",
    "                if date_str not in records:\n",
    "                    records[date_str] = {}\n",
    "                records[date_str][var] = value\n",
    "    \n",
    "        # Convert dict -> DataFrame\n",
    "        df = pd.DataFrame.from_dict(records, orient='index')\n",
    "        df.index = pd.to_datetime(df.index, format=\"%Y%m%d\")  # Proper date parsing\n",
    "        df.index.name = \"date\"\n",
    "        df.reset_index(inplace=True)\n",
    "        \n",
    "        # Add metadata\n",
    "        df['latitude'] = lat\n",
    "        df['longitude'] = lon\n",
    "\n",
    "        columns = [\"date\",\"temp_max\",\"temp_min\",\"humidity_specific\",\"pressure\",\"precipitation_total\",\"solar_radiation\",\"wind_speed\",\"lat\",\"lon\"]\n",
    "        df.columns = columns\n",
    "        print(f\"‚úÖ Retrieved {len(df)} daily records\")\n",
    "        return df\n",
    "\n",
    "    def get_bulk_data(self, locations, start_date, end_date, variables=None):\n",
    "        \"\"\"\n",
    "        Fetch data for multiple locations.\n",
    "\n",
    "        Args:\n",
    "            locations (list): [(lat, lon), ...]\n",
    "            start_date (str): YYYY-MM-DD\n",
    "            end_date (str): YYYY-MM-DD\n",
    "            variables (list): Variables list\n",
    "\n",
    "        Returns:\n",
    "            dict: { \"lat_lon\": DataFrame }\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for lat, lon in locations:\n",
    "            df = self.get_data(lat, lon, start_date, end_date, variables)\n",
    "            if not df.empty:\n",
    "                key = f\"lat_{lat}_lon_{lon}\"\n",
    "                results[key] = df\n",
    "        return results\n",
    "\n",
    "    def to_csv(self, data, filename):\n",
    "        \"\"\"Export single DataFrame or dict of DataFrames to CSV\"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            all_data = []\n",
    "            for loc, df in data.items():\n",
    "                df_copy = df.copy()\n",
    "                df_copy['location'] = loc\n",
    "                all_data.append(df_copy)\n",
    "            combined = pd.concat(all_data, ignore_index=True)\n",
    "            combined.to_csv(filename, index=False)\n",
    "        else:\n",
    "            data.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_location_by_address(self, address):\n",
    "        \"\"\"Return location data from an address, retrying if failed.\"\"\"\n",
    "        time.sleep(1)\n",
    "        geolocator = Nominatim(user_agent=\"gldas_fetcher\")\n",
    "        try:\n",
    "            return geolocator.geocode(address).raw\n",
    "        except:\n",
    "            return self.get_location_by_address(address)  # Recursive retry\n",
    "\n",
    "\n",
    "    def main():\n",
    "        \"\"\"Example usage with NASA data\"\"\"\n",
    "        \n",
    "        print(\"üöÄ NASA GLDAS Data Fetcher\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        \n",
    "        username = \"mahmoudmo12\"\n",
    "        password = \"Mahmoudmetawe12@\"\n",
    "        \n",
    "    \n",
    "        # Initialize with real credentials\n",
    "        fetcher = GLDASFetcher(username=username, password=password)\n",
    "        city = input(\"enter the city: \")\n",
    "        \n",
    "        \n",
    "        location = fetcher.get_location_by_address(city)\n",
    "        lat = location[\"lat\"]\n",
    "        lon = location[\"lon\"]\n",
    "        city_name = location['display_name']\n",
    "        #start_date = input(\"enter the start date formula (yyyy-mm-dd): \")\n",
    "        #end_date = input(\"enter the end date formula (yyyy-mm-dd): \")\n",
    "        \n",
    "        # Test with a single location\n",
    "        print(f\"\\nüåç Fetching DAILY data for {city_name}\")\n",
    "        \n",
    "        data = fetcher.get_data(\n",
    "            lat=lat,\n",
    "            lon=lon,\n",
    "            start_date=\"2000-01-01\",\n",
    "            end_date=\"2025-09-20\",\n",
    "            variables=['temp', 'humidity', 'pressure', 'precipitation', 'solar_rad', 'wind_speed']\n",
    "        )\n",
    "        \n",
    "        if not data.empty:\n",
    "            print(f\"\\n‚úÖ SUCCESS! Retrieved {len(data)} real data points\")\n",
    "            print(f\"üìã Columns: {list(data.columns)}\")\n",
    "            \n",
    "            # Export real data\n",
    "            fetcher.to_csv(data, \"nasa_daily_weather_data.csv\")\n",
    "            # import the data\n",
    "            df = pd.read_csv(\"nasa_daily_weather_data.csv\")\n",
    "        else:\n",
    "            print(\"‚ùå\")     \n",
    "        return df \n",
    "\n",
    "\n",
    "\n",
    "class WeatherForecaster:\n",
    "    def __init__(self, data_path, pipeline_path=\"pipeline_ensemble.pkl\"):\n",
    "        self.data_path = data_path\n",
    "        self.pipeline_path = pipeline_path\n",
    "        self.df, _ = self._wrangle(data_path, 'humidity_specific')\n",
    "        self.columns = self.df.columns\n",
    "        self.targets = self.columns.drop([\"date\", \"day_of_year\", \"lat\", \"lon\", \"did_rain\"])\n",
    "        self.models = None\n",
    "        self.pipeline = None\n",
    "        self.best_threshold = None\n",
    "\n",
    "    #wrangle without noise columns\n",
    "    def _wrangle(self,path,target=None):\n",
    "        \n",
    "        df = pd.read_csv(path)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        day_of_year = df['date'].dt.dayofyear\n",
    "        df['day_of_year'] = day_of_year\n",
    "        # getting the target column with the date for the model \n",
    "        df_prophet = df[['date', target]].rename(columns={'date': 'ds', target: 'y'})\n",
    "\n",
    "        did_rain = [0 if i < 0.2 else 1 for i in df['precipitation_total']]\n",
    "        df['did_rain'] = did_rain   \n",
    "        \n",
    "         # the nan val in nasa api is -999\n",
    "        df.replace(-999.0000,np.nan,inplace=True)\n",
    "        df_prophet.replace(-999.0000,np.nan,inplace=True)\n",
    "        #drop noise\n",
    "        df.drop(columns=[\"wind_speed\",\"precipitation_total\"],inplace=True)\n",
    "        # drop nulls\n",
    "        \n",
    "        df.dropna(inplace=True)\n",
    "        df_prophet.dropna(inplace=True)\n",
    "        \n",
    "        return df,df_prophet\n",
    "\n",
    "    def train_and_save_prophet(self):\n",
    "        for target in self.targets:\n",
    "            _, df_prophet = self._wrangle(self.data_path, target)\n",
    "            model = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "            model.fit(df_prophet)\n",
    "            filename = f'prophet_model_{target}.pkl'\n",
    "            with open(filename, 'wb') as file:\n",
    "                pk.dump(model, file)\n",
    "            print(f\"model for {target} is saved\")\n",
    "\n",
    "    def gathering_models(self):\n",
    "        all_models = []\n",
    "        for target in self.targets:\n",
    "            filename = f'prophet_model_{target}.pkl'\n",
    "            with open(filename, 'rb') as file:\n",
    "                all_models.append(pk.load(file))\n",
    "        self.models = all_models\n",
    "        return all_models\n",
    "\n",
    "    def predict_func(self, start_date, interval):\n",
    "        \n",
    "        \"\"\"\n",
    "        Internal helper to forecast all weather variables for a given time interval.\n",
    "\n",
    "        Args:\n",
    "            start_date (str): The starting date in 'YYYY-MM-DD' format.\n",
    "            interval (int): The number of future days to forecast.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the forecasted weather variables,\n",
    "                          ready to be used by the rain prediction pipeline.\n",
    "        \"\"\"\n",
    "        # 1. Create a DataFrame with the future dates for Prophet\n",
    "        future_dates_df = pd.DataFrame({\n",
    "            'ds': pd.date_range(start=start_date, periods=interval, freq='D')\n",
    "        })\n",
    "\n",
    "        # 2. Start building the final DataFrame, beginning with the date column\n",
    "        future_weather_df = pd.DataFrame({'date': future_dates_df['ds']})\n",
    "        # 3. Loop through each Prophet model to forecast its specific weather variable\n",
    "        #    This assumes self.targets and self.models are in the same order.\n",
    "        print(\"Forecasting future weather conditions...\")\n",
    "        for target_variable, model in zip(self.targets, self.models):\n",
    "            # Use the model to predict values for the entire date range\n",
    "            forecast = model.predict(future_dates_df)\n",
    "\n",
    "            # Extract the forecasted values ('yhat') and the dates ('ds')\n",
    "            future_values = forecast[['ds', 'yhat']].rename(\n",
    "                columns={'ds': 'date', 'yhat': target_variable}\n",
    "            )\n",
    "\n",
    "            # Merge this forecast into our main weather DataFrame\n",
    "            future_weather_df = pd.merge(future_weather_df, future_values, on='date')\n",
    "        \n",
    "        # 4. Ensure the column order matches exactly what the pipeline was trained on\n",
    "        required_columns = ['date', 'temp_max', 'temp_min', 'humidity_specific', 'pressure', 'solar_radiation']\n",
    "        future_weather_df = future_weather_df[required_columns]\n",
    "\n",
    "        print(\"Weather forecast complete.\")\n",
    "        return future_weather_df\n",
    "\n",
    "    class ProphetWrapper(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, model):\n",
    "            self.model = model\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            future = pd.DataFrame({'ds': X.flatten()})\n",
    "            forecast = self.model.predict(future)\n",
    "            return forecast[['yhat']].values\n",
    "\n",
    "    def save_pipeline(self):\n",
    "        \n",
    "        # 1. Define your new feature set (after creating lags, etc.)\n",
    "        # The date is still needed for Prophet, other features for XGBoost\n",
    "        date_feature = ['date'] \n",
    "        weather_features = ['temp_max', 'temp_min', 'humidity_specific', 'pressure', \"solar_radiation\"] \n",
    "        \n",
    "        # 2. Create the Prophet forecasting part (same as your code)\n",
    "        p_models = self.models\n",
    "        wrapped_prophets = [\n",
    "            (f'prophet_{i}', self.ProphetWrapper(model))\n",
    "            for i, model in enumerate(p_models)]\n",
    "        prophet_forecasters = FeatureUnion(wrapped_prophets)\n",
    "        prophet_pipeline = Pipeline([\n",
    "            ('selector', ColumnTransformer([('date_selector', 'passthrough', [0])])), # Select only the date column\n",
    "            ('prophet_features', prophet_forecasters)\n",
    "        ])\n",
    "        # Features and target\n",
    "        X = self.df.drop(columns='did_rain')\n",
    "        y = self.df['did_rain'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        neg, pos = np.bincount(y)   # count 0s and 1s in did_rain\n",
    "        imbalance_ratio = neg / pos\n",
    "        # 3. Create a pipeline for your standard numerical weather features\n",
    "        weather_pipeline = Pipeline([\n",
    "            ('selector', ColumnTransformer([('weather_selector', 'passthrough', [i for i, col in enumerate(X.columns) if col in weather_features])])),\n",
    "            ('scaler', StandardScaler()) # Scaling is good practice for XGBoost\n",
    "        ])\n",
    "        \n",
    "        # 4. Combine them using FeatureUnion\n",
    "        combined_features = FeatureUnion([\n",
    "            ('prophet_pipeline', prophet_pipeline),\n",
    "            ('weather_pipeline', weather_pipeline)\n",
    "        ])\n",
    "        \n",
    "        # 5. Create the final, complete pipeline\n",
    "        final_pipeline = Pipeline([\n",
    "            ('features', combined_features),\n",
    "            ('xgb_classifier', XGBClassifier(n_estimators=550,\n",
    "                                             max_depth=8,\n",
    "                                             learning_rate=0.05,\n",
    "                                             subsample=0.7,\n",
    "                                             scale_pos_weight=imbalance_ratio,\n",
    "                                             random_state=42)) # Your XGBoost model here\n",
    "        ])\n",
    "        #over_sampler = RandomOverSampler(sampling_strategy=\"all\",random_state=42)\n",
    "        #X_train_over,y_train_over = over_sampler.fit_resample(X_train,y_train)\n",
    "        # Now fit this final_pipeline on your data that includes ALL columns\n",
    "        final_pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        joblib.dump(final_pipeline, self.pipeline_path)\n",
    "\n",
    "        # Evaluation \n",
    "        y_pred = final_pipeline.predict(X_test)\n",
    "        y_proba = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        # After fitting the model and getting y_proba\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "        \n",
    "        # Calculate F1 score for each threshold\n",
    "        # Add a small epsilon to avoid division by zero :1e-9\n",
    "        f1_scores = 2 * recall * precision / (recall + precision + 1e-9)\n",
    "        \n",
    "        # Find the threshold that gives the best F1 score\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        \n",
    "        print(f\"Best Threshold for F1 Score: {best_threshold}\")\n",
    "        \n",
    "        self.best_threshold = best_threshold\n",
    "        \n",
    "        y_pred = (y_proba >= best_threshold).astype(int)\n",
    "        \n",
    "        print(\"\\nüìä Model Evaluation on Test Set\")\n",
    "        \n",
    "\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "        print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "        print(\"PR AUC:\", pr_auc)\n",
    "        \n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred)) \n",
    "    def calculate_metrics(self,test, train, model):     \n",
    "        # predictions\n",
    "        train_pred = model.predict(train)\n",
    "        test_pred = model.predict(test)\n",
    "    \n",
    "        y_true = test['y'].values\n",
    "        y_pred = test_pred['yhat'].values\n",
    "    \n",
    "        y_train = train['y'].values\n",
    "        yhat_train = train_pred['yhat'].values\n",
    "    \n",
    "        smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
    "    \n",
    "        metrics = {\n",
    "            'MAE': mean_absolute_error(y_true, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'R2': r2_score(y_true, y_pred),\n",
    "            \"SMAPE\": smape,\n",
    "            'Mean_Error': np.mean(y_pred - y_true),  # Bias\n",
    "            'Training MAE': mean_absolute_error(y_train, yhat_train)\n",
    "        }\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.2f}\")\n",
    "\n",
    "        return metrics\n",
    "    # def model(self):\n",
    "    #     df,_ = self._wrangle(self.data_path, \"temp_max\")\n",
    "        \n",
    "    #     self.pipeline = joblib.load(self.pipeline_path)\n",
    "    #     interval = int(input(\"enter the wanted interval (for 1 day =1): \"))\n",
    "    #     date = input(\"enter the date (YYYY-MM-DD): \")\n",
    "    #     date = pd.to_datetime(date)\n",
    "    #     predict_df, future_dates_df = self.predict_func(date,interval)\n",
    "    #     '''\n",
    "    #     temp_max_q = df['temp_max'].quantile(0.8)\n",
    "    #     temp_min_q = df['temp_min'].quantile(0.2)\n",
    "    #     humidity_specific_q = df['humidity_specific'].quantile(0.9)\n",
    "    #     solar_radiation_q = df['solar_radiation'].quantile(0.9)\n",
    "    #     wind_speed_q = df['wind_speed'].quantile(0.9)\n",
    "        \n",
    "    #     # Recommendations\n",
    "    #     recs = pd.Series({\n",
    "    #         \"temp_max\": \"too hot\" if predict_df.loc['temp_max'].values[0] > temp_max_q else \"normal\",\n",
    "    #         \"temp_min\": \"too cold\" if predict_df.loc['temp_min'].values[0] < temp_min_q else \"normal\",\n",
    "    #         \"humidity_specific\": \"humid\" if predict_df.loc['humidity_specific'].values[0] > humidity_specific_q else \"dry\",\n",
    "    #         \"pressure\": \"\",\n",
    "    #         \"precipitation_total\": \"\",\n",
    "    #         \"solar_radiation\": \"use sunscreen\" if predict_df.loc['solar_radiation'].values[0] > solar_radiation_q else \"safe\",\n",
    "    #         \"wind_speed\": \"windy\" if predict_df.loc['wind_speed'].values[0] > wind_speed_q else \"calm\"\n",
    "    #     })\n",
    "\n",
    "    #     predict_df['recomendations'] = recs\n",
    "    #     '''\n",
    "        \n",
    "    #     #future_dates = pd.date_range(date, periods=interval, freq=\"D\")\n",
    "    #     # future_input = np.array(future_dates).reshape(-1, 1)\n",
    "\n",
    "        \n",
    "    #     predict_df = predict_df.transpose()\n",
    "    #     predict_df.columns = ['date','temp_max','temp_min','humidity_specific','pressure','solar_radiation']\n",
    "    #     predictions = self.pipeline.predict_proba(predict_df).round(2) * 100\n",
    "    #     predict_df = predict_df.transpose()\n",
    "    #     #indexes = ['date','Maximum Temperature ','Minimum Temperature','Specific humidity','Pressure','Solar radiation']#,'Total precipitation',,'Wind speed'\n",
    "    #     #predict_df.index = indexes\n",
    "    #     proba_df = predictions\n",
    "    #     #proba_df = pd.DataFrame(predictions.round(2), \n",
    "    #     #                        columns=[\"probability of no rain\", \"probability of rain\"],index=future_dates)\n",
    "    #     #proba_df[\"rec\"] = proba_df['probability of rain'].apply(\n",
    "    #     #    lambda x: \"take care\" if x >= 50 else \"have a beautiful day\"\n",
    "    #     #)\n",
    "    #     return proba_df, predict_df\n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        Main function to generate a rain and weather forecast for a specified interval.\n",
    "        \"\"\"\n",
    "        # 1. Load the main rain prediction pipeline\n",
    "        self.pipeline = joblib.load(self.pipeline_path)\n",
    "        \n",
    "        # 2. Get the date and interval from the user\n",
    "        start_date = input(\"Enter the start date (YYYY-MM-DD): \")\n",
    "        interval = int(input(\"Enter the number of days to forecast: \"))\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # 3. Call the helper function to get the weather forecast for the full interval.\n",
    "        #    This `weather_preds_df` contains all the features needed for the next step.\n",
    "        weather_preds_df = self.predict_func(start_date, interval)\n",
    "\n",
    "        # 4. Use the complete weather forecast to predict the probability of rain.\n",
    "        #    The pipeline receives a DataFrame with multiple rows and 6 columns, just as it expects.\n",
    "        rain_probabilities = self.pipeline.predict_proba(weather_preds_df)\n",
    "\n",
    "        # 5. Format the rain probability results into a clean DataFrame\n",
    "        proba_df = pd.DataFrame(\n",
    "            (rain_probabilities * 100).round(2),\n",
    "            columns=[\"Prob. of No Rain (%)\", \"Prob. of Rain (%)\"],\n",
    "            index=weather_preds_df['date']  # Use the future dates as the index\n",
    "        )\n",
    "        \n",
    "        best_threshold_percent = self.best_threshold * 100\n",
    "        if best_threshold_percent> 50 :\n",
    "            proba_df[\"Recommendation\"] = proba_df['Prob. of Rain (%)'].apply(\n",
    "            lambda x: \"Take an umbrella! ‚òî\" if x >= 50 else \"Enjoy the clear skies! ‚òÄÔ∏è\")\n",
    "        else :     \n",
    "            proba_df[\"Recommendation\"] = proba_df['Prob. of Rain (%)'].apply(\n",
    "                lambda x: \"Take an umbrella! ‚òî\" if x >= best_threshold_percent else \"Enjoy the clear skies! ‚òÄÔ∏è\")\n",
    "        \n",
    "\n",
    "        # 7. Return both the rain probabilities and the detailed weather predictions\n",
    "        return proba_df, weather_preds_df.set_index('date')\n",
    "    \n",
    "    def Prophet_metrics(self):\n",
    "        # metrics calculation for each model of the prophets models\n",
    "        test_list = []\n",
    "        train_list = []\n",
    "        \n",
    "        for i in self.targets:\n",
    "            _,df_prophet = self._wrangle(\"nasa_daily_weather_data.csv\",i)\n",
    "        \n",
    "            # Split the data for the prophet models metrics (temp,humidity,...)\n",
    "            train_df, test_df = train_test_split(df_prophet, test_size=0.2, shuffle=False)\n",
    "        \n",
    "            train_list.append(train_df)\n",
    "        \n",
    "            test_list.append(test_df)\n",
    "        models = self.gathering_models()\n",
    "        for i in range(len(self.targets)):\n",
    "            print(\"=\"*70)\n",
    "            print(f\"Evaluation Results {self.targets[i]}\")\n",
    "            wf.calculate_metrics(test_list[i],train_list[i],models[i])\n",
    "            print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ NASA GLDAS Data Fetcher\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter the city:  ÿ¥ÿ®ŸäŸÜ ÿßŸÑŸÉŸàŸÖ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåç Fetching DAILY data for ÿ¥ÿ®ŸäŸÜ ÿßŸÑŸÉŸàŸÖ, ÿßŸÑŸÖŸÜŸàŸÅŸäÿ©, 32514, ŸÖÿµÿ±\n",
      "üåç Fetching NASA POWER data for (30.5545106, 31.0097923) from 2000-01-01 to 2025-09-20 ...\n",
      "‚úÖ Retrieved 9395 daily records\n",
      "\n",
      "‚úÖ SUCCESS! Retrieved 9395 real data points\n",
      "üìã Columns: ['date', 'temp_max', 'temp_min', 'humidity_specific', 'pressure', 'precipitation_total', 'solar_radiation', 'wind_speed', 'lat', 'lon']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:54:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:54:19 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model for temp_max is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:54:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:54:22 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model for temp_min is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:54:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:54:26 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model for humidity_specific is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:54:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:54:30 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model for pressure is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:54:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:54:33 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model for solar_radiation is saved\n",
      "Best Threshold for F1 Score: 0.2839652895927429\n",
      "\n",
      "üìä Model Evaluation on Test Set\n",
      "Accuracy: 0.8978179882916445\n",
      "Precision: 0.5446428571428571\n",
      "Recall: 0.5754716981132075\n",
      "F1 Score: 0.5596330275229358\n",
      "ROC AUC: 0.8546422790913516\n",
      "PR AUC: 0.5485301384279062\n",
      "Confusion Matrix:\n",
      " [[1565  102]\n",
      " [  90  122]]\n",
      "======================================================================\n",
      "Evaluation Results temp_max\n",
      "MAE: 2.20\n",
      "RMSE: 2.93\n",
      "R2: 0.86\n",
      "SMAPE: 7.69\n",
      "Mean_Error: -0.09\n",
      "Training MAE: 2.22\n",
      "======================================================================\n",
      "======================================================================\n",
      "Evaluation Results temp_min\n",
      "MAE: 1.42\n",
      "RMSE: 1.84\n",
      "R2: 0.90\n",
      "SMAPE: 10.81\n",
      "Mean_Error: -0.04\n",
      "Training MAE: 1.38\n",
      "======================================================================\n",
      "======================================================================\n",
      "Evaluation Results humidity_specific\n",
      "MAE: 0.90\n",
      "RMSE: 1.15\n",
      "R2: 0.79\n",
      "SMAPE: 11.18\n",
      "Mean_Error: -0.01\n",
      "Training MAE: 0.88\n",
      "======================================================================\n",
      "======================================================================\n",
      "Evaluation Results pressure\n",
      "MAE: 0.22\n",
      "RMSE: 0.29\n",
      "R2: 0.66\n",
      "SMAPE: 0.22\n",
      "Mean_Error: 0.00\n",
      "Training MAE: 0.23\n",
      "======================================================================\n",
      "======================================================================\n",
      "Evaluation Results solar_radiation\n",
      "MAE: 0.40\n",
      "RMSE: 0.58\n",
      "R2: 0.90\n",
      "SMAPE: 7.89\n",
      "Mean_Error: -0.01\n",
      "Training MAE: 0.41\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the start date (YYYY-MM-DD):  2025-10-24\n",
      "Enter the number of days to forecast:  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Forecasting future weather conditions...\n",
      "Weather forecast complete.\n",
      "\n",
      "Weather predictions:\n",
      "             temp_max   temp_min  humidity_specific    pressure  \\\n",
      "date                                                              \n",
      "2025-10-24  31.887798  18.636365           9.974379  101.161615   \n",
      "2025-10-25  31.710680  18.512232           9.918459  101.166520   \n",
      "2025-10-26  31.536464  18.386188           9.863247  101.171615   \n",
      "2025-10-27  31.365020  18.258371           9.808737  101.176934   \n",
      "2025-10-28  31.196127  18.128915           9.754884  101.182506   \n",
      "2025-10-29  31.029474  17.997942           9.701601  101.188350   \n",
      "2025-10-30  30.864675  17.865567           9.648770  101.194483   \n",
      "\n",
      "            solar_radiation  \n",
      "date                         \n",
      "2025-10-24         4.584926  \n",
      "2025-10-25         4.539375  \n",
      "2025-10-26         4.495129  \n",
      "2025-10-27         4.452241  \n",
      "2025-10-28         4.410738  \n",
      "2025-10-29         4.370624  \n",
      "2025-10-30         4.331877  \n",
      "\n",
      "Rain probabilities:\n",
      "            Prob. of No Rain (%)  Prob. of Rain (%)             Recommendation\n",
      "date                                                                          \n",
      "2025-10-24             99.099998               0.90  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-10-25             98.570000               1.43  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-10-26             99.489998               0.51  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-10-27             98.750000               1.25  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-10-28             99.419998               0.58  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-10-29             99.410004               0.59  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-10-30             99.660004               0.34  Enjoy the clear skies! ‚òÄÔ∏è\n"
     ]
    }
   ],
   "source": [
    "#fetch the data according to the location \n",
    "GLDASFetcher.main()\n",
    "\n",
    "wf = WeatherForecaster(\"nasa_daily_weather_data.csv\")\n",
    "\n",
    "# Train Prophet models and save them\n",
    "wf.train_and_save_prophet()\n",
    "\n",
    "# Load Prophet models into memory\n",
    "wf.gathering_models()\n",
    "\n",
    "# Save ensemble pipeline\n",
    "wf.save_pipeline()\n",
    "\n",
    "wf.Prophet_metrics()\n",
    "# Make predictions\n",
    "proba_df, weather_preds = wf.model()\n",
    "\n",
    "print(\"\\nWeather predictions:\")\n",
    "print(weather_preds)\n",
    "print(\"\\nRain probabilities:\")\n",
    "print(proba_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_max</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>humidity_specific</th>\n",
       "      <th>pressure</th>\n",
       "      <th>solar_radiation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-10-24</th>\n",
       "      <td>31.887798</td>\n",
       "      <td>18.636365</td>\n",
       "      <td>9.974379</td>\n",
       "      <td>101.161615</td>\n",
       "      <td>4.584926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-25</th>\n",
       "      <td>31.710680</td>\n",
       "      <td>18.512232</td>\n",
       "      <td>9.918459</td>\n",
       "      <td>101.166520</td>\n",
       "      <td>4.539375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-26</th>\n",
       "      <td>31.536464</td>\n",
       "      <td>18.386188</td>\n",
       "      <td>9.863247</td>\n",
       "      <td>101.171615</td>\n",
       "      <td>4.495129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-27</th>\n",
       "      <td>31.365020</td>\n",
       "      <td>18.258371</td>\n",
       "      <td>9.808737</td>\n",
       "      <td>101.176934</td>\n",
       "      <td>4.452241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-28</th>\n",
       "      <td>31.196127</td>\n",
       "      <td>18.128915</td>\n",
       "      <td>9.754884</td>\n",
       "      <td>101.182506</td>\n",
       "      <td>4.410738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-29</th>\n",
       "      <td>31.029474</td>\n",
       "      <td>17.997942</td>\n",
       "      <td>9.701601</td>\n",
       "      <td>101.188350</td>\n",
       "      <td>4.370624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-30</th>\n",
       "      <td>30.864675</td>\n",
       "      <td>17.865567</td>\n",
       "      <td>9.648770</td>\n",
       "      <td>101.194483</td>\n",
       "      <td>4.331877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             temp_max   temp_min  humidity_specific    pressure  \\\n",
       "date                                                              \n",
       "2025-10-24  31.887798  18.636365           9.974379  101.161615   \n",
       "2025-10-25  31.710680  18.512232           9.918459  101.166520   \n",
       "2025-10-26  31.536464  18.386188           9.863247  101.171615   \n",
       "2025-10-27  31.365020  18.258371           9.808737  101.176934   \n",
       "2025-10-28  31.196127  18.128915           9.754884  101.182506   \n",
       "2025-10-29  31.029474  17.997942           9.701601  101.188350   \n",
       "2025-10-30  30.864675  17.865567           9.648770  101.194483   \n",
       "\n",
       "            solar_radiation  \n",
       "date                         \n",
       "2025-10-24         4.584926  \n",
       "2025-10-25         4.539375  \n",
       "2025-10-26         4.495129  \n",
       "2025-10-27         4.452241  \n",
       "2025-10-28         4.410738  \n",
       "2025-10-29         4.370624  \n",
       "2025-10-30         4.331877  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import warnings\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pickle as pk\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, precision_recall_curve, auc, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"This Pipeline instance is not fitted yet\")\n",
    "\n",
    "class GLDASFetcher:\n",
    "    def __init__(self, username=None, password=None):\n",
    "        \"\"\"\n",
    "        Initialize GLDAS/POWER fetcher (username/password only needed for GLDAS OPeNDAP, \n",
    "        not for NASA POWER API).\n",
    "        \"\"\"\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "\n",
    "        # GLDAS variable names mapped to NASA POWER parameters\n",
    "        self.variables_map = {\n",
    "            'temp': ['T2M_MAX', 'T2M_MIN'],           # Daily max/min temp\n",
    "            'humidity': ['QV2M'],                     # Specific humidity\n",
    "            'pressure': ['PS'],                       # Surface pressure\n",
    "            'precipitation': ['PRECTOTCORR'],         # Corrected daily precipitation\n",
    "            'solar_rad': ['ALLSKY_SFC_SW_DWN'],       # Solar radiation\n",
    "            'wind_speed': ['WS2M']                    # Wind speed\n",
    "        }\n",
    "\n",
    "    def get_data(self, lat, lon, start_date, end_date, variables=None):\n",
    "        \"\"\"\n",
    "        Fetch daily NASA POWER data for a given location & date range.\n",
    "        \"\"\"\n",
    "        variables = variables or list(self.variables_map.keys())\n",
    "    \n",
    "        base_url = \"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
    "    \n",
    "        # Collect POWER parameter codes\n",
    "        power_params = []\n",
    "        for var in variables:\n",
    "            power_params.extend(self.variables_map.get(var, []))\n",
    "    \n",
    "        params = {\n",
    "            'parameters': ','.join(power_params),\n",
    "            'community': 'RE',\n",
    "            'longitude': lon,\n",
    "            'latitude': lat,\n",
    "            'start': start_date.replace(\"-\", \"\"),\n",
    "            'end': end_date.replace(\"-\", \"\"),\n",
    "            'format': 'JSON'\n",
    "        }\n",
    "    \n",
    "        print(f\"üåç Fetching NASA POWER data for ({lat}, {lon}) from {start_date} to {end_date} ...\")\n",
    "        response = requests.get(base_url, params=params, timeout=60)\n",
    "    \n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå API error {response.status_code}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "        data = response.json()['properties']['parameter']\n",
    "    \n",
    "        # Build dataframe manually\n",
    "        records = {}\n",
    "        for var, timeseries in data.items():\n",
    "            for date_str, value in timeseries.items():\n",
    "                if date_str not in records:\n",
    "                    records[date_str] = {}\n",
    "                records[date_str][var] = value\n",
    "    \n",
    "        # Convert dict -> DataFrame\n",
    "        df = pd.DataFrame.from_dict(records, orient='index')\n",
    "        df.index = pd.to_datetime(df.index, format=\"%Y%m%d\")  # Proper date parsing\n",
    "        df.index.name = \"date\"\n",
    "        df.reset_index(inplace=True)\n",
    "        \n",
    "        # Add metadata\n",
    "        df['latitude'] = lat\n",
    "        df['longitude'] = lon\n",
    "\n",
    "        columns = [\"date\",\"temp_max\",\"temp_min\",\"humidity_specific\",\"pressure\",\"precipitation_total\",\"solar_radiation\",\"wind_speed\",\"lat\",\"lon\"]\n",
    "        df.columns = columns\n",
    "        print(f\"‚úÖ Retrieved {len(df)} daily records\")\n",
    "        return df\n",
    "\n",
    "    def get_bulk_data(self, locations, start_date, end_date, variables=None):\n",
    "        \"\"\"\n",
    "        Fetch data for multiple locations.\n",
    "\n",
    "        Args:\n",
    "            locations (list): [(lat, lon), ...]\n",
    "            start_date (str): YYYY-MM-DD\n",
    "            end_date (str): YYYY-MM-DD\n",
    "            variables (list): Variables list\n",
    "\n",
    "        Returns:\n",
    "            dict: { \"lat_lon\": DataFrame }\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for lat, lon in locations:\n",
    "            df = self.get_data(lat, lon, start_date, end_date, variables)\n",
    "            if not df.empty:\n",
    "                key = f\"lat_{lat}_lon_{lon}\"\n",
    "                results[key] = df\n",
    "        return results\n",
    "\n",
    "    def to_csv(self, data, filename):\n",
    "        \"\"\"Export single DataFrame or dict of DataFrames to CSV\"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            all_data = []\n",
    "            for loc, df in data.items():\n",
    "                df_copy = df.copy()\n",
    "                df_copy['location'] = loc\n",
    "                all_data.append(df_copy)\n",
    "            combined = pd.concat(all_data, ignore_index=True)\n",
    "            combined.to_csv(filename, index=False)\n",
    "        else:\n",
    "            data.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_location_by_address(self, address):\n",
    "        \"\"\"Return location data from an address, retrying if failed.\"\"\"\n",
    "        time.sleep(1)\n",
    "        geolocator = Nominatim(user_agent=\"gldas_fetcher\")\n",
    "        try:\n",
    "            return geolocator.geocode(address).raw\n",
    "        except:\n",
    "            return self.get_location_by_address(address)  # Recursive retry\n",
    "\n",
    "\n",
    "    def main():\n",
    "        \"\"\"Example usage with NASA data\"\"\"\n",
    "        \n",
    "        print(\"üöÄ NASA GLDAS Data Fetcher\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        \n",
    "        username = \"mahmoudmo12\"\n",
    "        password = \"Mahmoudmetawe12@\"\n",
    "        \n",
    "    \n",
    "        # Initialize with real credentials\n",
    "        fetcher = GLDASFetcher(username=username, password=password)\n",
    "        city = input(\"enter the city: \")\n",
    "        \n",
    "        \n",
    "        location = fetcher.get_location_by_address(city)\n",
    "        lat = location[\"lat\"]\n",
    "        lon = location[\"lon\"]\n",
    "        city_name = location['display_name']\n",
    "        #start_date = input(\"enter the start date formula (yyyy-mm-dd): \")\n",
    "        #end_date = input(\"enter the end date formula (yyyy-mm-dd): \")\n",
    "        \n",
    "        # Test with a single location\n",
    "        print(f\"\\nüåç Fetching DAILY data for {city_name}\")\n",
    "        \n",
    "        data = fetcher.get_data(\n",
    "            lat=lat,\n",
    "            lon=lon,\n",
    "            start_date=\"2000-01-01\",\n",
    "            end_date=\"2025-09-20\",\n",
    "            variables=['temp', 'humidity', 'pressure', 'precipitation', 'solar_rad', 'wind_speed']\n",
    "        )\n",
    "        \n",
    "        if not data.empty:\n",
    "            print(f\"\\n‚úÖ SUCCESS! Retrieved {len(data)} real data points\")\n",
    "            print(f\"üìã Columns: {list(data.columns)}\")\n",
    "            \n",
    "            # Export real data\n",
    "            fetcher.to_csv(data, \"nasa_daily_weather_data.csv\")\n",
    "            # import the data\n",
    "            df = pd.read_csv(\"nasa_daily_weather_data.csv\")\n",
    "        else:\n",
    "            print(\"‚ùå\")     \n",
    "        return df \n",
    "\n",
    "\n",
    "\n",
    "class WeatherForecaster:\n",
    "    def __init__(self, data_path, pipeline_path=\"pipeline_ensemble.pkl\"):\n",
    "        self.data_path = data_path\n",
    "        self.pipeline_path = pipeline_path\n",
    "        self.df, _ = self._wrangle(data_path, 'humidity_specific')\n",
    "        self.columns = self.df.columns\n",
    "        self.targets = self.columns.drop([\"date\", \"day_of_year\", \"lat\", \"lon\", \"did_rain\"])\n",
    "        self.models = None\n",
    "        self.pipeline = None\n",
    "        self.best_threshold = None\n",
    "\n",
    "    #wrangle without noise columns\n",
    "    def _wrangle(self,path,target=None):\n",
    "        \n",
    "        df = pd.read_csv(path)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        day_of_year = df['date'].dt.dayofyear\n",
    "        df['day_of_year'] = day_of_year\n",
    "        # getting the target column with the date for the model \n",
    "        df_prophet = df[['date', target]].rename(columns={'date': 'ds', target: 'y'})\n",
    "\n",
    "        did_rain = [0 if i < 0.2 else 1 for i in df['precipitation_total']]\n",
    "        df['did_rain'] = did_rain   \n",
    "        \n",
    "         # the nan val in nasa api is -999\n",
    "        df.replace(-999.0000,np.nan,inplace=True)\n",
    "        df_prophet.replace(-999.0000,np.nan,inplace=True)\n",
    "        #drop noise\n",
    "        df.drop(columns=[\"wind_speed\",\"precipitation_total\"],inplace=True)\n",
    "        # drop nulls\n",
    "        \n",
    "        df.dropna(inplace=True)\n",
    "        df_prophet.dropna(inplace=True)\n",
    "        \n",
    "        return df,df_prophet\n",
    "\n",
    "    def train_and_save_prophet(self):\n",
    "        for target in self.targets:\n",
    "            _, df_prophet = self._wrangle(self.data_path, target)\n",
    "            model = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "            model.fit(df_prophet)\n",
    "            filename = f'prophet_model_{target}.pkl'\n",
    "            with open(filename, 'wb') as file:\n",
    "                pk.dump(model, file)\n",
    "            print(f\"model for {target} is saved\")\n",
    "\n",
    "    def gathering_models(self):\n",
    "        all_models = []\n",
    "        for target in self.targets:\n",
    "            filename = f'prophet_model_{target}.pkl'\n",
    "            with open(filename, 'rb') as file:\n",
    "                all_models.append(pk.load(file))\n",
    "        self.models = all_models\n",
    "        return all_models\n",
    "\n",
    "    def predict_func(self, start_date, interval):\n",
    "        \n",
    "        \"\"\"\n",
    "        Internal helper to forecast all weather variables for a given time interval.\n",
    "\n",
    "        Args:\n",
    "            start_date (str): The starting date in 'YYYY-MM-DD' format.\n",
    "            interval (int): The number of future days to forecast.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the forecasted weather variables,\n",
    "                          ready to be used by the rain prediction pipeline.\n",
    "        \"\"\"\n",
    "        # 1. Create a DataFrame with the future dates for Prophet\n",
    "        future_dates_df = pd.DataFrame({\n",
    "            'ds': pd.date_range(start=start_date, periods=interval, freq='D')\n",
    "        })\n",
    "\n",
    "        # 2. Start building the final DataFrame, beginning with the date column\n",
    "        future_weather_df = pd.DataFrame({'date': future_dates_df['ds']})\n",
    "        # 3. Loop through each Prophet model to forecast its specific weather variable\n",
    "        #    This assumes self.targets and self.models are in the same order.\n",
    "        print(\"Forecasting future weather conditions...\")\n",
    "        for target_variable, model in zip(self.targets, self.models):\n",
    "            # Use the model to predict values for the entire date range\n",
    "            forecast = model.predict(future_dates_df)\n",
    "\n",
    "            # Extract the forecasted values ('yhat') and the dates ('ds')\n",
    "            future_values = forecast[['ds', 'yhat']].rename(\n",
    "                columns={'ds': 'date', 'yhat': target_variable}\n",
    "            )\n",
    "\n",
    "            # Merge this forecast into our main weather DataFrame\n",
    "            future_weather_df = pd.merge(future_weather_df, future_values, on='date')\n",
    "        \n",
    "        # 4. Ensure the column order matches exactly what the pipeline was trained on\n",
    "        required_columns = ['date', 'temp_max', 'temp_min', 'humidity_specific', 'pressure', 'solar_radiation']\n",
    "        future_weather_df = future_weather_df[required_columns]\n",
    "\n",
    "        print(\"Weather forecast complete.\")\n",
    "        return future_weather_df\n",
    "\n",
    "    class ProphetWrapper(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, model):\n",
    "            self.model = model\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            future = pd.DataFrame({'ds': X.flatten()})\n",
    "            forecast = self.model.predict(future)\n",
    "            return forecast[['yhat']].values\n",
    "\n",
    "    def save_pipeline(self):\n",
    "        \n",
    "        # 1. Define your new feature set (after creating lags, etc.)\n",
    "        # The date is still needed for Prophet, other features for XGBoost\n",
    "        date_feature = ['date'] \n",
    "        weather_features = ['temp_max', 'temp_min', 'humidity_specific', 'pressure', \"solar_radiation\"] \n",
    "        \n",
    "        # 2. Create the Prophet forecasting part (same as your code)\n",
    "        p_models = self.models\n",
    "        wrapped_prophets = [\n",
    "            (f'prophet_{i}', self.ProphetWrapper(model))\n",
    "            for i, model in enumerate(p_models)]\n",
    "        prophet_forecasters = FeatureUnion(wrapped_prophets)\n",
    "        prophet_pipeline = Pipeline([\n",
    "            ('selector', ColumnTransformer([('date_selector', 'passthrough', [0])])), # Select only the date column\n",
    "            ('prophet_features', prophet_forecasters)\n",
    "        ])\n",
    "        # Features and target\n",
    "        X = self.df.drop(columns='did_rain')\n",
    "        y = self.df['did_rain'].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "        \n",
    "        neg, pos = np.bincount(y)   # count 0s and 1s in did_rain\n",
    "        imbalance_ratio = neg / pos\n",
    "        # 3. Create a pipeline for your standard numerical weather features\n",
    "        weather_pipeline = Pipeline([\n",
    "            ('selector', ColumnTransformer([('weather_selector', 'passthrough', [i for i, col in enumerate(X.columns) if col in weather_features])])),\n",
    "            ('scaler', StandardScaler()) # Scaling is good practice for XGBoost\n",
    "        ])\n",
    "        \n",
    "        # 4. Combine them using FeatureUnion\n",
    "        combined_features = FeatureUnion([\n",
    "            ('prophet_pipeline', prophet_pipeline),\n",
    "            ('weather_pipeline', weather_pipeline)\n",
    "        ])\n",
    "        \n",
    "        # 5. Create the final, complete pipeline\n",
    "        final_pipeline = Pipeline([\n",
    "            ('features', combined_features),\n",
    "            ('xgb_classifier', GradientBoostingClassifier(\n",
    "                n_estimators=450,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                random_state=42\n",
    "            )) # Your XGBoost model here\n",
    "        ])\n",
    "        over_sampler = RandomOverSampler(sampling_strategy=\"all\",random_state=42)\n",
    "        X_train_over,y_train_over = over_sampler.fit_resample(X_train,y_train)\n",
    "        # Now fit this final_pipeline on your data that includes ALL columns\n",
    "        final_pipeline.fit(X_train_over, y_train_over)\n",
    "        \n",
    "        joblib.dump(final_pipeline, self.pipeline_path)\n",
    "\n",
    "        # Evaluation \n",
    "        y_pred = final_pipeline.predict(X_test)\n",
    "        y_proba = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        # After fitting the model and getting y_proba\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "        \n",
    "        # Calculate F1 score for each threshold\n",
    "        # Add a small epsilon to avoid division by zero :1e-9\n",
    "        f1_scores = 2 * recall * precision / (recall + precision + 1e-9)\n",
    "        \n",
    "        # Find the threshold that gives the best F1 score\n",
    "        best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "        \n",
    "        print(f\"Best Threshold for F1 Score: {best_threshold}\")\n",
    "        \n",
    "        self.best_threshold = best_threshold\n",
    "        \n",
    "        y_pred = (y_proba >= best_threshold).astype(int)\n",
    "        \n",
    "        print(\"\\nüìä Model Evaluation on Test Set\")\n",
    "        \n",
    "\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "        print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "        print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "        print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "        print(\"PR AUC:\", pr_auc)\n",
    "        \n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred)) \n",
    "    def calculate_metrics(self,test, train, model):     \n",
    "        # predictions\n",
    "        train_pred = model.predict(train)\n",
    "        test_pred = model.predict(test)\n",
    "    \n",
    "        y_true = test['y'].values\n",
    "        y_pred = test_pred['yhat'].values\n",
    "    \n",
    "        y_train = train['y'].values\n",
    "        yhat_train = train_pred['yhat'].values\n",
    "    \n",
    "        smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
    "    \n",
    "        metrics = {\n",
    "            'MAE': mean_absolute_error(y_true, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'R2': r2_score(y_true, y_pred),\n",
    "            \"SMAPE\": smape,\n",
    "            'Mean_Error': np.mean(y_pred - y_true),  # Bias\n",
    "            'Training MAE': mean_absolute_error(y_train, yhat_train)\n",
    "        }\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.2f}\")\n",
    "\n",
    "        return metrics\n",
    "    # def model(self):\n",
    "    #     df,_ = self._wrangle(self.data_path, \"temp_max\")\n",
    "        \n",
    "    #     self.pipeline = joblib.load(self.pipeline_path)\n",
    "    #     interval = int(input(\"enter the wanted interval (for 1 day =1): \"))\n",
    "    #     date = input(\"enter the date (YYYY-MM-DD): \")\n",
    "    #     date = pd.to_datetime(date)\n",
    "    #     predict_df, future_dates_df = self.predict_func(date,interval)\n",
    "    #     '''\n",
    "    #     temp_max_q = df['temp_max'].quantile(0.8)\n",
    "    #     temp_min_q = df['temp_min'].quantile(0.2)\n",
    "    #     humidity_specific_q = df['humidity_specific'].quantile(0.9)\n",
    "    #     solar_radiation_q = df['solar_radiation'].quantile(0.9)\n",
    "    #     wind_speed_q = df['wind_speed'].quantile(0.9)\n",
    "        \n",
    "    #     # Recommendations\n",
    "    #     recs = pd.Series({\n",
    "    #         \"temp_max\": \"too hot\" if predict_df.loc['temp_max'].values[0] > temp_max_q else \"normal\",\n",
    "    #         \"temp_min\": \"too cold\" if predict_df.loc['temp_min'].values[0] < temp_min_q else \"normal\",\n",
    "    #         \"humidity_specific\": \"humid\" if predict_df.loc['humidity_specific'].values[0] > humidity_specific_q else \"dry\",\n",
    "    #         \"pressure\": \"\",\n",
    "    #         \"precipitation_total\": \"\",\n",
    "    #         \"solar_radiation\": \"use sunscreen\" if predict_df.loc['solar_radiation'].values[0] > solar_radiation_q else \"safe\",\n",
    "    #         \"wind_speed\": \"windy\" if predict_df.loc['wind_speed'].values[0] > wind_speed_q else \"calm\"\n",
    "    #     })\n",
    "\n",
    "    #     predict_df['recomendations'] = recs\n",
    "    #     '''\n",
    "        \n",
    "    #     #future_dates = pd.date_range(date, periods=interval, freq=\"D\")\n",
    "    #     # future_input = np.array(future_dates).reshape(-1, 1)\n",
    "\n",
    "        \n",
    "    #     predict_df = predict_df.transpose()\n",
    "    #     predict_df.columns = ['date','temp_max','temp_min','humidity_specific','pressure','solar_radiation']\n",
    "    #     predictions = self.pipeline.predict_proba(predict_df).round(2) * 100\n",
    "    #     predict_df = predict_df.transpose()\n",
    "    #     #indexes = ['date','Maximum Temperature ','Minimum Temperature','Specific humidity','Pressure','Solar radiation']#,'Total precipitation',,'Wind speed'\n",
    "    #     #predict_df.index = indexes\n",
    "    #     proba_df = predictions\n",
    "    #     #proba_df = pd.DataFrame(predictions.round(2), \n",
    "    #     #                        columns=[\"probability of no rain\", \"probability of rain\"],index=future_dates)\n",
    "    #     #proba_df[\"rec\"] = proba_df['probability of rain'].apply(\n",
    "    #     #    lambda x: \"take care\" if x >= 50 else \"have a beautiful day\"\n",
    "    #     #)\n",
    "    #     return proba_df, predict_df\n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        Main function to generate a rain and weather forecast for a specified interval.\n",
    "        \"\"\"\n",
    "        # 1. Load the main rain prediction pipeline\n",
    "        self.pipeline = joblib.load(self.pipeline_path)\n",
    "        \n",
    "        # 2. Get the date and interval from the user\n",
    "        start_date = input(\"Enter the start date (YYYY-MM-DD): \")\n",
    "        interval = int(input(\"Enter the number of days to forecast: \"))\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # 3. Call the helper function to get the weather forecast for the full interval.\n",
    "        #    This `weather_preds_df` contains all the features needed for the next step.\n",
    "        weather_preds_df = self.predict_func(start_date, interval)\n",
    "\n",
    "        # 4. Use the complete weather forecast to predict the probability of rain.\n",
    "        #    The pipeline receives a DataFrame with multiple rows and 6 columns, just as it expects.\n",
    "        rain_probabilities = self.pipeline.predict_proba(weather_preds_df)\n",
    "\n",
    "        # 5. Format the rain probability results into a clean DataFrame\n",
    "        proba_df = pd.DataFrame(\n",
    "            (rain_probabilities * 100).round(2),\n",
    "            columns=[\"Prob. of No Rain (%)\", \"Prob. of Rain (%)\"],\n",
    "            index=weather_preds_df['date']  # Use the future dates as the index\n",
    "        )\n",
    "        \n",
    "        best_threshold_percent = self.best_threshold * 100\n",
    "        if best_threshold_percent> 50 :\n",
    "            proba_df[\"Recommendation\"] = proba_df['Prob. of Rain (%)'].apply(\n",
    "            lambda x: \"Take an umbrella! ‚òî\" if x >= 50 else \"Enjoy the clear skies! ‚òÄÔ∏è\")\n",
    "        else :     \n",
    "            proba_df[\"Recommendation\"] = proba_df['Prob. of Rain (%)'].apply(\n",
    "                lambda x: \"Take an umbrella! ‚òî\" if x >= best_threshold_percent else \"Enjoy the clear skies! ‚òÄÔ∏è\")\n",
    "        \n",
    "\n",
    "        # 7. Return both the rain probabilities and the detailed weather predictions\n",
    "        return proba_df, weather_preds_df.set_index('date')\n",
    "    \n",
    "    def Prophet_metrics(self):\n",
    "        # metrics calculation for each model of the prophets models\n",
    "        test_list = []\n",
    "        train_list = []\n",
    "        \n",
    "        for i in self.targets:\n",
    "            _,df_prophet = self._wrangle(\"nasa_daily_weather_data.csv\",i)\n",
    "        \n",
    "            # Split the data for the prophet models metrics (temp,humidity,...)\n",
    "            train_df, test_df = train_test_split(df_prophet, test_size=0.2, shuffle=False)\n",
    "        \n",
    "            train_list.append(train_df)\n",
    "        \n",
    "            test_list.append(test_df)\n",
    "        models = self.gathering_models()\n",
    "        for i in range(len(self.targets)):\n",
    "            print(\"=\"*70)\n",
    "            print(f\"Evaluation Results {self.targets[i]}\")\n",
    "            self.calculate_metrics(test_list[i],train_list[i],models[i])\n",
    "            print(\"=\"*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ NASA GLDAS Data Fetcher\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter the city:  ÿ≥ÿ±ÿ≥ ÿßŸÑŸÑŸäÿßŸÜ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåç Fetching DAILY data for ÿ≥ÿ±ÿ≥ ÿßŸÑŸÑŸäÿßŸÜ, ÿßŸÑŸÖŸÜŸàŸÅŸäÿ©, 32861, ŸÖÿµÿ±\n",
      "üåç Fetching NASA POWER data for (30.4439168, 30.9707264) from 2000-01-01 to 2025-09-20 ...\n",
      "‚úÖ Retrieved 9395 daily records\n",
      "\n",
      "‚úÖ SUCCESS! Retrieved 9395 real data points\n",
      "üìã Columns: ['date', 'temp_max', 'temp_min', 'humidity_specific', 'pressure', 'precipitation_total', 'solar_radiation', 'wind_speed', 'lat', 'lon']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:07:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:07:25 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model for temp_max is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:07:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:07:28 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model for temp_min is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:07:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:07:33 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model for humidity_specific is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:07:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:07:37 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model for pressure is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:07:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:07:41 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model for solar_radiation is saved\n",
      "Best Threshold for F1 Score: 0.9436699501760559\n",
      "\n",
      "üìä Model Evaluation on Test Set\n",
      "Accuracy: 0.9058009579563597\n",
      "Precision: 0.5916230366492147\n",
      "Recall: 0.5330188679245284\n",
      "F1 Score: 0.5607940446650124\n",
      "ROC AUC: 0.8376758610542042\n",
      "PR AUC: 0.546198143111335\n",
      "Confusion Matrix:\n",
      " [[1589   78]\n",
      " [  99  113]]\n",
      "======================================================================\n",
      "Evaluation Results temp_max\n",
      "MAE: 2.20\n",
      "RMSE: 2.93\n",
      "R2: 0.86\n",
      "SMAPE: 7.69\n",
      "Mean_Error: -0.09\n",
      "Training MAE: 2.22\n",
      "======================================================================\n",
      "======================================================================\n",
      "Evaluation Results temp_min\n",
      "MAE: 1.42\n",
      "RMSE: 1.84\n",
      "R2: 0.90\n",
      "SMAPE: 10.81\n",
      "Mean_Error: -0.04\n",
      "Training MAE: 1.38\n",
      "======================================================================\n",
      "======================================================================\n",
      "Evaluation Results humidity_specific\n",
      "MAE: 0.90\n",
      "RMSE: 1.15\n",
      "R2: 0.79\n",
      "SMAPE: 11.18\n",
      "Mean_Error: -0.01\n",
      "Training MAE: 0.88\n",
      "======================================================================\n",
      "======================================================================\n",
      "Evaluation Results pressure\n",
      "MAE: 0.22\n",
      "RMSE: 0.29\n",
      "R2: 0.66\n",
      "SMAPE: 0.22\n",
      "Mean_Error: 0.00\n",
      "Training MAE: 0.23\n",
      "======================================================================\n",
      "======================================================================\n",
      "Evaluation Results solar_radiation\n",
      "MAE: 0.38\n",
      "RMSE: 0.58\n",
      "R2: 0.90\n",
      "SMAPE: 7.60\n",
      "Mean_Error: -0.00\n",
      "Training MAE: 0.39\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the start date (YYYY-MM-DD):  2025-11-10\n",
      "Enter the number of days to forecast:  9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Forecasting future weather conditions...\n",
      "Weather forecast complete.\n",
      "\n",
      "Weather predictions:\n",
      "             temp_max   temp_min  humidity_specific    pressure  \\\n",
      "date                                                              \n",
      "2025-11-10  29.027938  16.337148           9.043086  101.278069   \n",
      "2025-11-11  28.846653  16.192700           8.980726  101.286327   \n",
      "2025-11-12  28.661837  16.047480           8.916572  101.294505   \n",
      "2025-11-13  28.473542  15.901540           8.850610  101.302558   \n",
      "2025-11-14  28.281925  15.754942           8.782867  101.310444   \n",
      "2025-11-15  28.087245  15.607758           8.713417  101.318125   \n",
      "2025-11-16  27.889854  15.460073           8.642374  101.325569   \n",
      "2025-11-17  27.690192  15.311982           8.569899  101.332750   \n",
      "2025-11-18  27.488772  15.163591           8.496191  101.339650   \n",
      "\n",
      "            solar_radiation  \n",
      "date                         \n",
      "2025-11-10         3.984087  \n",
      "2025-11-11         3.951561  \n",
      "2025-11-12         3.918956  \n",
      "2025-11-13         3.886232  \n",
      "2025-11-14         3.853371  \n",
      "2025-11-15         3.820381  \n",
      "2025-11-16         3.787291  \n",
      "2025-11-17         3.754154  \n",
      "2025-11-18         3.721044  \n",
      "\n",
      "Rain probabilities:\n",
      "            Prob. of No Rain (%)  Prob. of Rain (%)             Recommendation\n",
      "date                                                                          \n",
      "2025-11-10                 89.47              10.53  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-11-11                 94.14               5.86  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-11-12                 37.78              62.22        Take an umbrella! ‚òî\n",
      "2025-11-13                 68.41              31.59  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-11-14                 77.78              22.22  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-11-15                 84.45              15.55  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-11-16                 65.12              34.88  Enjoy the clear skies! ‚òÄÔ∏è\n",
      "2025-11-17                 38.26              61.74        Take an umbrella! ‚òî\n",
      "2025-11-18                 31.87              68.13        Take an umbrella! ‚òî\n"
     ]
    }
   ],
   "source": [
    "#fetch the data according to the location \n",
    "GLDASFetcher.main()\n",
    "\n",
    "wf1 = WeatherForecaster(\"nasa_daily_weather_data.csv\")\n",
    "\n",
    "# Train Prophet models and save them\n",
    "wf1.train_and_save_prophet()\n",
    "\n",
    "# Load Prophet models into memory\n",
    "wf1.gathering_models()\n",
    "\n",
    "# Save ensemble pipeline\n",
    "wf1.save_pipeline()\n",
    "\n",
    "wf1.Prophet_metrics()\n",
    "# Make predictions\n",
    "proba_df, weather_preds = wf1.model()\n",
    "\n",
    "print(\"\\nWeather predictions:\")\n",
    "print(weather_preds)\n",
    "print(\"\\nRain probabilities:\")\n",
    "print(proba_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
